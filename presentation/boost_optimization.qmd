---
title: "boost_optimizing_engine"
subtitle: "by Pete Barwis"
format: 
  revealjs:
    theme: 
      - simple
title-slide-attributes:
  data-background-image: assets/img/hsd_logo.png
  data-background-size: 20%
  data-background-position: bottom 50px center
execute: 
  warning: false
  message: false
  echo: false
---

## Research questions

1.  How can we optimize boosts to get rides claimed earlier?
2.  How can we optimize boosts to reduce the average cost of rides?
3.  How can we both minimize trip claim time and minimize average cost of rides?\

## Theory

-   Theoretical assumptions
    -   Claim time: is maximized by the incentive to wait for larger boosts
    -   Boost size: *Drivers* are price sensitive, some more than others

## Hypotheses (alternative)

-   Claim time:
    -   *H1*: Switching from escalating boosts to *one‑time, limited‑time* boosts will shorten claim time.
    -   *H2*: Making boost offers earlier by using unclaimed predictions will shorten claim time.
-   Boost size:
    -   *H3*: Personalizing the boost to the driver’s price sensitivity will lower average boost price.

## What are we trying to do?

1.  Driver model
    -   Identifies drivers in synthetic trip data so we can influence their behavior
2.  Early warning model
    -   Flags trips that will go unclaimed so we can act before the wait clock starts
3.  Price sensitivity model
    -   Predicts the probability each driver will claim a specific trip across a range of boost amounts

## How will we do it?

::: {style="font-size: 90%;"}
1.  Understand the available trip data
2.  Fit driver model(s) and predict driver data
3.  Fit the unclaimed ride model and predict unclaimed rides
4.  Fit the price sensitivity model
5.  Predict the optimal (minimal) boost amount for each driver that will move that trip over the claim probability threshold
6.  Compare optimized boost amounts against actual amounts
:::

## Understanding trip data

```{r}
library(tidyverse)
library(scales)
library(knitr)
library(modelsummary)

# Load data
d <- read_csv("../input/boost_df.csv", show_col_types = FALSE)


writeLines(paste("How many trip observations?", scales::comma(length(d$trip_id))))

writeLines(paste("How many unique trips?", scales::comma(length(unique(d$trip_id)))))

writeLines(paste("What % of observations in the trip data are duplicate trip records?", percent(sum(duplicated(d$trip_id)) / length(d$trip_id) )))

writeLines("How do trips break down by geographic area?")
sort(table(d$origin_metro_area_name))
```

## Understanding boost offers

```{r}

writeLines("What are average boost offer amounts by sequence?")
d %>%
  group_by(seq_boost_count) %>%
  summarize(average_cumulative_boost = mean(cumulative_boost_amount_cents, na.rm = TRUE) / 100) %>%
  mutate(as.factor(seq_boost_count)) %>%
  ggplot(., aes(x = seq_boost_count, y = average_cumulative_boost)) +
  geom_bar(stat = "identity", position = "dodge", fill = "#F57E44") +
  scale_x_continuous("Boost Sequence") + 
  scale_y_continuous("Avg Cumulative Boost Amount", labels = scales::dollar_format()) + 
  theme_minimal()

writeLines("What does the distribution of boosts look like?")
table(d$seq_boost_count)

```

## Synthetic driver model

::: {style="font-size: 70%;"}
-   We need driver data to model price sensitivity by driver. None is provided, so we will predict some.
-   The idea is to find trips that are most similar within metro area, and assign a synthetic driver id to those trips, assuming the same driver claims the most similar trips.
-   Create an analytic (numeric) dataset useful for locating similar trips by engineering new features including a numeric day of week, numeric start and end times, numeric metro number, and numeric trip starts during peak hours.
-   Remove observations that aren't completed trips, so boosted trip records that were not claimed are removed, leaving only claimed trips.
-   Loop over metro area, defining a conditional number of clusters within that area relative to the number of trips, and using k-means cluster analysis to assign cluster (aka driver) id numbers to each completed trip.
-   The result is a set of driver ids, each with multiple trips within each metro area, which have some covariance that may be useful for predictive modeling.
:::

## K-Means driver assignment example

![](assets/img/cluster_driver_example.png){fig-align="center"}

## Synthetic driver data covariance

::: {style="font-size: 60%;"}
How much of the variation in the driver data is explained by the driver-level? If we pick two trips at random, how much of the difference in each feature is because they belong to different drivers?
:::

```{r}
icc_plot <- read_rds("assets/img/icc_plot.rds")
icc_plot
```

## Predict unclaimed rides

::: {style="font-size: 60%;"}
-   If we predict unclaimed rides at the time of trip creation, we could offer boosts before wait time becomes worrisome.

-   To do that, we need to predict trips as they are measured at the time of creation, with limited information.

-   Remove leakage, which is information that is related to/explains the outcome that isn't known at the time of prediction (claimed_at, unclaimed_at, dollars_paid_to_driver, commute_minutes, etc).

-   Limit the trip data to only the first observation, before boosts are added.

-   Convert predictors to numeric, split data into train/test sets, fit a Random Forest classification model that predicts `ever_unclaimed`.

-   The Random Forest model performs better than XGBoost on this problem, but the overall model fit is poor.

-   To try to make this model as useful as possible for this exercise, given its limitations, I refit the model on the full training set, and predict all trips in the training set (in-sample prediction), using the model fit to the same data.

-   Use these early warning unclaimed ride predictions as the trips to optimize using a boost optimization method.
:::

## Time potentially saved

How much time could we save by predicting unclaimed rides and making boost offers earlier?

::: {style="font-size: 60%;"}
The following shows the distribution of time elapsed before an initial boost offer is made. Theoretically, this elapsed time could be reduced to one day, if predictions are made daily.
:::

::::: columns
::: {.column width="50%"}
```{r}
# Calculate average elapsed time from trip creation and first boost offer.
# Most of this elapsed time could be eliminated if boost offers were made as 
# soon as a model predicts it will go unclaimed.
d_first_boost_offered <- d %>%
  arrange(trip_id, seq_boost_count) %>%
  filter(seq_boost_count == 1) %>%
  select(created_at, boost_timestamp) %>%
  mutate(days_to_first_boost = as.numeric(difftime(boost_timestamp, 
                                                   created_at, units = "days")))

t <- days_to_first_boost_summary <- d_first_boost_offered %>%
  summarize(min_days = min(days_to_first_boost),
            mean_days = mean(days_to_first_boost),
            max_days = max(days_to_first_boost)) %>%
  kable(.)

print(t)

```
:::

::: {.column width="50%"}
```{r}
#| fig-height: 5
ggplot(d_first_boost_offered, aes(x = days_to_first_boost)) +
  geom_histogram(fill = "#F57E44", color = "grey") +
  theme_minimal()
```
:::
:::::

## Unclaimed model analytic dataset

::: {style="font-size: 50%;"}
```{r}
read_rds("assets/d_unclaimed.rds")
```
:::

## Unclaimed model performance

::: {style="font-size: 80%;"}
-   These performance statistics are calculated from 1/5 test set.
-   Performance is mediocre here, but in real life, HopSkipDrive's additional rider and trip location data would very likely produce a powerful model.
:::

```{r}
unclaimed_auc <- read_csv("assets/unclaimed_auc.csv")
unclaimed_accuracy <- read_csv("assets/unclaimed_accuracy.csv")
print(bind_rows(unclaimed_auc, unclaimed_accuracy))
print(read_rds("assets/unclaimed_conf_mat.rds"))
```

## Model driver price sensitivity

-   The idea is to model driver price sensitivity, and then apply that model to find the driver with the lowest boost price for a trip that still increases the probability of claiming that trip above the threshold (0.5)
-   To do this, we fit a nonparametric predictive model to boosted trips, predicting the likelihood each trip will be claimed, given boosted offer amounts and other trip predictors.
-   With enough trees, a nonparametric model will identify varying amounts of price sensitivity for each driver.

## Price sensitivity model analytic dataset

::: {style="font-size: 50%;"}
```{r}

read_rds("assets/d_boost.rds")
```
:::

## Price sensitivity model performance

::: {style="font-size: 80%;"}
-   These performance statistics are calculated from 1/5 test set.
-   Performance is mediocre here, but in real life, HopSkipDrive's additional rider and trip location data would very likely produce a powerful model.
:::

```{r}
boosted_auc <- read_csv("assets/boosted_auc.csv")
boosted_accuracy <- read_csv("assets/boosted_accuracy.csv")
print(bind_rows(boosted_auc, boosted_accuracy))
print(read_rds("assets/boosted_conf_mat.rds"))
```

## Optimize boost amounts

::: {style="font-size: 80%;"}
-   Create a simulated dataset that contains, for a given metro area, every possible combination of drivers, trips, and 10 different boost offers (at deciles of actual boost distribution).
-   Apply the predictive model to each metro area dataset and predict how likely each driver is to claim each trip given the boost offer size.
-   Filter the results by selecting only trips where the prediction exceeds the threshold, and sort the remaining by smallest boost size, then highest probability.
-   The result is a list of drivers joined to the trips that they are likely to claim given the smallest possible boost size.
-   This method could be used iteratively to select the next most price sensitive set of drivers, requiring a slightly greater boost offer amount, to support the one-time, limited-time offer approach.
:::

## Compare actual with optimized boosts

::: {style="font-size: 70%;"}
Distribution of Actual versus Optimized Boost Offers: Optimized offers cluster much closer to zero.
:::

![](assets/img/p_actual_predicted_boost_dist.png)

## Distribution of boost cost savings

::: {style="font-size: 70%;"}
Distribution of Actual Minus Optimized Boost Offers: Blue positive values indicate the amount of money likely saved with optimization.
:::

![](assets/img/p_actual_predicted_boost_difference.png){width="16.97in"}

```{r}
print(read_rds("assets/summary_optim_boosts.rds"))
```

## Main recommendations

-   Predicting rides that are likely to go unclaimed could enable boost offers to be made 80 days earlier on average.

-   Boost offers, if optimized according to driver price sensitivity, could be reduced by \$6 on average.

-   Making one-time, limited-time boost offers using a driver-specific price sensitivity model removes the incentive to wait, adds an incentive to act, and provides fair compensation to the driver.

## Future work

-   Adding additional driver, rider, location, and historical boost data will make for better models.

-   We need to solve the cold start problem for new drivers with no price sensitivity data, possibly using a collaborative filtering-style approach.

-   Additional analysis of individual driver price sensitivity is warranted. A log-linear model for each driver might be more persuasive, but would require more data. The nonparametric model does not always show an increase in probability when there is an increase in boost offer size.

## Confidence in approach

-   I believe the approach itself is sound, but real life constraints and incomplete information on my side may have missed some hurdles to implementation.

## Deploying to production

::: {style="font-size: 80%;"}
-   Use local or SageMaker env to preprocess training data, iteratively fit sci-kit models, and evaluate performance. This becomes the model training source code.
-   Store models as compressed pickle files on S3
-   Write scheduled Python job in DAG orchestrator (Airflow, Dagster, etc.) that runs daily. The job:
    -   Queries and preprocesses prediction data using same logic as model training source

    -   Loads stored compressed models from S3

    -   Applies models to prediction data and generates predictions

    -   Writes predictions to S3 or inserts into db or warehouse

    -   Engineering apps retrieve predictions and use in production
:::

## Delivering this at scale

-   Optimize the optimization method.

-   Scale up models with additional data and complexity.

-   Implement batch jobs to generate predictions and boost recommendations, and load into warehouse/db.

-   Small-scale beta testing, using experimentation or post-hoc analysis to compare the performance of old and new methods.

-   Have a backup plan if all one-time, limited-time offers fail to produce a claimed ride.

# \~ Thank you! \~
